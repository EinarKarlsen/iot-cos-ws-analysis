{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala notebook for analyzing archived IoT event data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Scala and Spark SQL to read and transform archived IoT event data. The purpose of the notebook is to analyze historic (i.e. archived) IoT Event Data comming from Elevators, and then determine at which dates specific elevators have been impacted by maintenance stops caused by overheated engines. \n",
    "\n",
    "The notebook has 3 sections:\n",
    "1. Connect to Cloud Object Store to read the archived IoT event data into a Spark SQL Data Frame.\n",
    "2. Filter and aggregate the event data so that it shows the dates where maintenance stops occured.\n",
    "3. Save the information to a DB2 data table for further analysis e.g. by Business Intelligence tools.\n",
    "\n",
    "Spark SQL is documented at: https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from Cloud Object Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial step creates the connection to the Cloud Object Storage and reads the data from the archived Cloudant NoSQL Database into a Spark SQL Data Frame. A a second step prints out the schema of the Data Frame as well as a couple of rows of event data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 72:>                                                         (0 + 2) / 2]"
     ]
    }
   ],
   "source": [
    "// Insert the connection to the file on Cloud Object Storage\n",
    "<here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// if this variable is true, the notebook will print out the intermediate results for debugging/testing purposes.\n",
    "val debug = false\n",
    "if (debug) { dfData1.show(5); dfData1.printSchema()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and aggregate event data using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial dataframe will be filtered by removing empty rows as wells as event data where the motor temperature is below 200 degree Fahrenheit. In a final step, the information will be aggregated and sorted to show just the elevator name, the date as well as the max motor temperature on the date where an issue occured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Filter out null rows\n",
    "val dfData2 = dfData1.filter(dfData1.col(\"key\").!==(\"null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Select just the required set of columns\n",
    "val dfData3 = dfData2.select(\"doc.deviceId\",\"doc.timestamp\",\"doc.data.d.motorTemp\")\n",
    "if (debug) {dfData3.show()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// Search for rows where the motor temperature went above 200 degree Fahrenheit\n",
    "val dfData4 = dfData3.filter(dfData1.col(\"doc.data.d.motorTemp\").>(200))\n",
    "if (debug) {dfData4.show()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Select the date from the timestamp and cast the resulting string to a proper date type\n",
    "val dfData5 = dfData4.withColumn(\"date\", dfData4.col(\"timestamp\").substr(0,10).cast(\"date\"))\n",
    "dfData5.cache()\n",
    "if (debug) {dfData5.printSchema(); dfData5.show()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Group by device id (elevator name) and date and determine the max temperatur for that date\n",
    "val dfData6 = dfData5.groupBy(\"deviceId\",\"date\").max(\"motorTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Order by date\n",
    "val dfData7 = dfData6.orderBy(dfData6.col(\"date\").asc)\n",
    "if (debug) {dfData7.printSchema(); dfData7.show()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to DB2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to DB2 is done in this notebook using a simple scheme that will create the database table and simply append the rows in the data frame to the table.\n",
    "\n",
    "The code has been inspired by the following two resources:\n",
    "1. http://bigdatums.net/2016/10/16/writing-to-a-database-from-spark/\n",
    "2. http://support.datascientistworkbench.com/knowledgebase/articles/829689-access-dashdb-or-db2-using-jdbc-from-scala-noteb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Insert the credentials for the Db2 service \n",
    "<here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute properties and URL for the connection\n",
    "val prop = new java.util.Properties\n",
    "prop.setProperty(\"driver\", \"com.ibm.db2.jcc.DB2Driver\")\n",
    "prop.setProperty(\"user\", credentials_1(\"user\"))\n",
    "prop.setProperty(\"password\", credentials_1(\"password\")) \n",
    " \n",
    "//jdbc mysql url - destination database is named \"data\"\n",
    "//val url = \"jdbc:mysql://localhost:3306/data\"\n",
    "val url = List(\"jdbc:db2://\", credentials_1(\"host\"), \":\", credentials_1(\"port\"), \"/\", credentials_1(\"database\")).mkString(\"\");\n",
    " \n",
    "//destination database table \n",
    "val table = \"sample_data_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 77:===================>                                      (1 + 2) / 3]"
     ]
    }
   ],
   "source": [
    "//write data from spark dataframe to database\n",
    "dfData7.write.mode(\"append\").jdbc(url, table, prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+                                          \n",
      "|  deviceId|      date|max(motorTemp)|\n",
      "+----------+----------+--------------+\n",
      "|Elevator01|2017-08-03|         220.0|\n",
      "|Elevator01|2017-08-07|         220.0|\n",
      "|Elevator03|2017-08-07|         213.0|\n",
      "+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfData7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.1",
   "language": "scala",
   "name": "scala-spark21"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
