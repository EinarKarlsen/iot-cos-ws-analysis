{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala notebook for analyzing archived IoT event data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Scala and Spark SQL to read and transform archived IoT event data. The purpose of the notebook is to analyze historic (i.e. archived) IoT Event Data comming from Elevators, and then determine at which dates specific elevators have been impacted by maintenance stops caused by overheated engines. \n",
    "\n",
    "The notebook has 3 sections:\n",
    "1. Connect to Cloud Object Store to read the archived IoT event data into a Spark SQL Data Frame.\n",
    "2. Filter and aggregate the event data so that it shows the dates where maintenance stops occured.\n",
    "3. Save the information to a DB2 data table for further analysis e.g. by Business Intelligence tools.\n",
    "\n",
    "Spark SQL is documented at: https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from Cloud Object Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial step creates the connection to the Cloud Object Storage and reads the data from the archived Cloudant NoSQL Database into a list of Spark SQL Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (1 + 1) / 2]"
     ]
    }
   ],
   "source": [
    "// Insert the connection to the file on Cloud Object Storage\n",
    "<here>\n",
    "\n",
    "val dfList1 = List(\"db05.json\",\"db07.json\",\"db08.json\",\"db09.json\",\"db10.json\").map(db => spark.read.json(cos.url(\"iotelevator-donotdelete-pr-dchdyhr697izhi\", db)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and aggregate event data using Spark SQLÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial dataframe will be filtered by removing empty rows as wells as event data where the motor temperature is below 200 degree Fahrenheit. In a final step, the information will be aggregated and sorted to show just the elevator name, the date as well as the max motor temperature on the date where an issue occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "object ElevatorEventDataTransformer {   \n",
    "    def transform(dfData: DataFrame): DataFrame = {\n",
    "      dfData.filter(dfData.col(\"key\").!==(\"null\"))\n",
    "            .select(\"doc.deviceId\",\"doc.timestamp\",\"doc.data.d.motorTemp\")\n",
    "            .filter(($\"doc.data.d.motorTemp\").>(200))\n",
    "            .withColumn(\"date\",($\"timestamp\").substr(0,10).cast(\"date\"))\n",
    "            .groupBy(\"deviceId\",\"date\").max(\"motorTemp\")\n",
    "            .orderBy(($\"date\").asc)\n",
    "    }\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 36:===========================================>              (3 + 1) / 4]+----------+----------+--------------+\n",
      "|  deviceId|      date|max(motorTemp)|\n",
      "+----------+----------+--------------+\n",
      "|Elevator01|2017-05-08|         214.0|\n",
      "|Elevator01|2017-07-24|         215.0|\n",
      "|Elevator01|2017-08-03|         220.0|\n",
      "|Elevator01|2017-08-07|         220.0|\n",
      "|Elevator03|2017-08-07|         213.0|\n",
      "+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val dfListT = dfList1.map(df => ElevatorEventDataTransformer.transform(df))\n",
    "val dfReduced = dfListT.reduce((df1,df2) => df1.union(df2))\n",
    "dfReduced.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to DB2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to DB2 is done in this notebook using a simple scheme that will create the database table and simply append the rows in the data frame to the table.\n",
    "\n",
    "The code has been inspired by the following two resources:\n",
    "1. http://bigdatums.net/2016/10/16/writing-to-a-database-from-spark/\n",
    "2. http://support.datascientistworkbench.com/knowledgebase/articles/829689-access-dashdb-or-db2-using-jdbc-from-scala-noteb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Insert the credentials for the Db2 service\n",
    "<here>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Compute properties and URL for the connection\n",
    "val prop = new java.util.Properties\n",
    "prop.setProperty(\"driver\", \"com.ibm.db2.jcc.DB2Driver\")\n",
    "prop.setProperty(\"user\", credentials_1(\"username\"))\n",
    "prop.setProperty(\"password\", credentials_1(\"password\")) \n",
    " \n",
    "//jdbc mysql url - destination database is named \"data\"\n",
    "//val url = \"jdbc:mysql://localhost:3306/data\"\n",
    "val url = List(\"jdbc:db2://\", credentials_1(\"host\"), \":\", credentials_1(\"port\"), \"/\", credentials_1(\"database\")).mkString(\"\");\n",
    " \n",
    "//destination database table \n",
    "val table = \"sample_data_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//write data from spark dataframe to database\n",
    "dfReduced.write.mode(\"overwrite\").jdbc(url, table, prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.1",
   "language": "scala",
   "name": "scala-spark21"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
